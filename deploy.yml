apiVersion: apps/v1
kind: Deployment
metadata:
  name: tinyllama
  labels:
    app: tinyllama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tinyllama
  template:
    metadata:
      labels:
        app: tinyllama
    spec:
      containers:
      - name: tinyllama
        image: python:3.9-slim
        command: ["/bin/bash", "-c"]
        args: 
          - pip install transformers torch; 
            python3 -c "from transformers import AutoModelForCausalLM, AutoTokenizer; 
            print('Loading TinyLlama model...');
            model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0'); 
            tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0'); 
            print('TinyLlama model loaded successfully')"
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "4"
            memory: 8Gi
---
apiVersion: v1
kind: Service
metadata:
  name: tinyllama-service
spec:
  selector:
    app: tinyllama
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
