apiVersion: apps/v1
kind: Deployment
metadata:
  name: tinyllama
  labels:
    app: tinyllama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tinyllama
  template:
    metadata:
      labels:
        app: tinyllama
    spec:
      containers:
      - name: tinyllama
        image: python:3.9-slim
        command: ["/bin/bash", "-c"]
        args: 
          - |
            pip install transformers torch flask gunicorn;
            cat > app.py << 'EOF'
            from flask import Flask, request, jsonify
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            app = Flask(__name__)
            print('Loading TinyLlama model...')
            model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')
            tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')
            print('TinyLlama model loaded successfully')

            @app.route('/generate', methods=['POST'])
            def generate():
                data = request.json
                prompt = data.get('prompt', '')
                max_length = data.get('max_length', 50)
                temperature = data.get('temperature', 1.0)

                # Format for chat
                chat_prompt = f"<human>: {prompt}\n<assistant>: "
                
                inputs = tokenizer(chat_prompt, return_tensors="pt")
                outputs = model.generate(
                    inputs["input_ids"],
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True
                )
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                # Remove the prompt format from response
                response = response.replace(chat_prompt, "")
                return jsonify({"prompt": prompt, "generated_text": response})

            @app.route('/health', methods=['GET'])
            def health():
                return jsonify({"status": "healthy"})

            if __name__ == '__main__':
                app.run(host='0.0.0.0', port=8000)
            EOF
            gunicorn --bind 0.0.0.0:8000 app:app
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "4"
            memory: 8Gi
---
apiVersion: v1
kind: Service
metadata:
  name: tinyllama-service
spec:
  selector:
    app: tinyllama
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
