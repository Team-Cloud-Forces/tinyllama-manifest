apiVersion: apps/v1
kind: Deployment
metadata:
  name: tinyllama
  labels:
    app: tinyllama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tinyllama
  template:
    metadata:
      labels:
        app: tinyllama
    spec:
      containers:
      - name: tinyllama
        image: python:3.9-slim
        command: ["/bin/bash", "-c"]
        args: 
          - |
            pip install transformers torch flask gunicorn;
            cat > app.py << 'EOF'
            from flask import Flask, request, jsonify
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            app = Flask(__name__)
            # Increase Flask's timeout
            app.config['TIMEOUT'] = 300  # 5 minutes in seconds

            print('Loading TinyLlama model...')
            model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')
            tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')
            print('TinyLlama model loaded successfully')

            # Perform model warmup
            print("Warming up the model with test inferences...")
            sample_prompts = [
                "Hello, how are you?",
                "What is machine learning?",
                "Tell me a short story."
            ]
            for prompt in sample_prompts:
                messages = [{"role": "user", "content": prompt}]
                chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                inputs = tokenizer(chat_prompt, return_tensors="pt")
                model.generate(
                    inputs["input_ids"],
                    max_new_tokens=50,
                    temperature=0.7,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95
                )
            print("Model warmup complete!")

            @app.route('/generate', methods=['POST'])
            def generate():
                data = request.json
                prompt = data.get('prompt', '')
                max_length = data.get('max_length', 512)
                temperature = data.get('temperature', 0.7)
                system_message = data.get('system_message', '')

                # Format messages using proper chat template
                messages = []
                if system_message:
                    messages.append({"role": "system", "content": system_message})
                messages.append({"role": "user", "content": prompt})
                
                # Use the tokenizer's chat template
                chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                
                # Calculate prompt length in tokens
                prompt_tokens = len(tokenizer(chat_prompt)['input_ids'])
                
                # Ensure max_length doesn't exceed context window
                available_tokens = 2048 - prompt_tokens
                if max_length > available_tokens:
                    max_length = available_tokens
                
                inputs = tokenizer(chat_prompt, return_tensors="pt")
                outputs = model.generate(
                    inputs["input_ids"],
                    max_new_tokens=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    repetition_penalty=1.1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                # Remove the prompt format from response
                response = response.replace(chat_prompt, "")
                return jsonify({"prompt": prompt, "generated_text": response})

            @app.route('/health', methods=['GET'])
            def health():
                return jsonify({"status": "healthy"})

            if __name__ == '__main__':
                app.run(host='0.0.0.0', port=8000)
            EOF
            gunicorn --bind 0.0.0.0:8000 --timeout 300 app:app
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "4"
            memory: 8Gi
---
apiVersion: v1
kind: Service
metadata:
  name: tinyllama-service
spec:
  selector:
    app: tinyllama
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
